{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5b60293",
   "metadata": {},
   "source": [
    "# Bayesian Hyperperameter Optimization\n",
    "\n",
    "## Objective\n",
    "This is an exercise in learning how to implement Baysian optimization for tuning the hyperparameters of a machine learning model.\n",
    "\n",
    "I tried to keep everything as simple as possible. Data engineering was kept to a minimum, and the basic model structure was very straightforward.\n",
    "\n",
    "## Results\n",
    "This notebook successfully achieves the following:\n",
    "1. Defines a generating function to construct a model with variable hyperparameters\n",
    "2. Defines an evaluation function that scores a model with variable hyperparameters\n",
    "3. Applies Bayesian optimization to probe a parameter space for potentially good hyperparameter values\n",
    "4. Allows for easy manipulation of several assumptionsm bounds, and settings for the optimization task\n",
    "5. Stores the progress of the optimizer so for reference or reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db15ab1e",
   "metadata": {},
   "source": [
    "# References and Data\n",
    "\n",
    "## References\n",
    "\n",
    "This code that follows was heavily inspired by/ taken from code published by Jeff Heaten for his course *T81 558:Applications of Deep Neural Networks*. It can be found on [github](https://github.com/jeffheaton/t81_558_deep_learning/tree/master)\n",
    "\n",
    "Here is the citation for that course:\n",
    "```\n",
    "cff-version: 1.2.0\n",
    "message: \"If you use this software, please cite it as below.\"\n",
    "authors:\n",
    "- family-names: \"Jeff\"\n",
    "  given-names: \"Heaton\"\n",
    "  orcid: \"https://orcid.org/0000-0003-1496-4049\"\n",
    "title: \"Applications of Deep Neural Networks\"\n",
    "version: 2021.08.01\n",
    "date-released: 2021-08-01\n",
    "url: \"https://arxiv.org/abs/2009.05673\"\n",
    "```\n",
    "\n",
    "## Data\n",
    "\n",
    "The dataset used to for demonstration purposes is the \"California Housing\" dataset found [here](https://www.kaggle.com/datasets/camnugent/california-housing-prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50df8233",
   "metadata": {},
   "source": [
    "# Loading Packages and Helper Functions\n",
    "\n",
    "There are a bunch of unused packages here, but my goal when making this notebook was to build a basic template that would be easy to adapt to future tasks.\n",
    "\n",
    "The `hms_string` function used to make runtime benchmark outputs more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6e7d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bayesian-optimization\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "import io\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import requests\n",
    "from scipy.stats import zscore\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.initializers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, InputLayer\n",
    "from tensorflow.keras.layers import LeakyReLU,PReLU\n",
    "from tensorflow.keras.optimizers.legacy import Adam # Note: legacy is for M1/M2 issues\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Nicely format a time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b76cdc7",
   "metadata": {},
   "source": [
    "# Preparing the Data\n",
    "\n",
    "For this dataset, I did the following preparation:\n",
    "1. Load the dataset from a csv file to a `pandas` dataframe\n",
    "2. Dropped rows with missing data (this only eliminates 270 of 20,640 rows)\n",
    "3. Identified a target variable: `median_house_value`\n",
    "4. Replaced the categorical feature, `ocean_proximity`, with dummies\n",
    "5. Normalized the non-categrical features using z-scores\n",
    "6. Stored the features and target as `numpy` arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d29d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "df = pd.read_csv('housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f64b4cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with NA values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d513c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of the target column (or columns)\n",
    "y_column = 'median_house_value'\n",
    "\n",
    "# Keep track of any categorical features to make dummies for\n",
    "categorical_features = ['ocean_proximity']\n",
    "\n",
    "# Normalize the values in non-target and non-categorical columns\n",
    "for feature in df.drop(y_column, axis=1).drop(categorical_features, axis=1).columns:\n",
    "    df[feature]=zscore(df[feature])\n",
    "\n",
    "# Create dummies for all of the categorical features\n",
    "# and drop the original feature column\n",
    "for feature in categorical_features:\n",
    "    dummies = pd.get_dummies(df[feature], drop_first=True, prefix=feature)\n",
    "    df = pd.concat([df,dummies],axis=1)\n",
    "    df.drop(feature, axis=1, inplace=True)\n",
    "\n",
    "# Keep track of the feature columns that will be used in the model\n",
    "x_columns = df.drop(y_column, axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c7c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# For regression or binary classfication:\n",
    "x = df[x_columns].values\n",
    "y = df[y_column].values\n",
    "####################\n",
    "# For multiclassification:\n",
    "#x = df[x_columns].values\n",
    "#dummies = pd.get_dummies(df[y_column])\n",
    "#y_columns = dummies.columns\n",
    "#y = dummies.values\n",
    "####################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcc578e",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization\n",
    "\n",
    "## Tools\n",
    "\n",
    "I want to use Bayesian optimization for tuning the parameters of my model. I will be using the `bays_opt` package for this task ([source](https://github.com/bayesian-optimization/BayesianOptimization)).\n",
    "\n",
    "At the very least, the optimizer needs two things:\n",
    "1. A \"black box\" funtion that provides an evaluation, or score, for a model with given parameter values\n",
    "2. The domain from which parameter values may be selected\n",
    "\n",
    "## Implementation and Settings\n",
    "\n",
    "The steps in this implementaion are:\n",
    "1. Write a function that will construct a model given a set of parameter values:\n",
    "    - `dropout_rate` - the percent of neurons to drop out between hidden layers\n",
    "    - `neuronPct` - used to set the number of neurons in the first hidden layer as `neuronPct * MAX_NEURONS`\n",
    "    - `neuronShrink` - used to control how quicky the number of neurons in each successive hidden layers drops as a percent \n",
    "        - e.g. if `neuronShrink=.7` then the number of neurons in each hidden layer will be 30% fewer than the previous layer\n",
    "2. Write a function that evaluates a model (gives a score) given a set of parameter values:\n",
    "    - `dropout_rate`\n",
    "    - `neuronPct`\n",
    "    - `neuronShrink` \n",
    "    - `learning_rate` - the learning rate for the `Adam` optimizer when compiling the model\n",
    "\n",
    "In this implementation, the Bayesian optimizer will vary these optimization parameters (`dropout_rate`, `neuronPct`, `neuronShrink`, `learning_rate`) to try and maximize the score for the model.\n",
    "\n",
    "We can set ranges for each of these parameters as a dictionary, e.g.:\n",
    "\n",
    "```\n",
    "pbounds = {'dropout_rate': (0.0, 0.499),\n",
    "           'learning_rate': (0.0, 0.1),\n",
    "           'neuronPct': (0.01, 1),\n",
    "           'neuronShrink': (0.01, 1)\n",
    "          }\n",
    "```\n",
    "\n",
    "Some important choices and settings determined manually, outside of these automatically tuned hyperparameters:\n",
    "- Constucting the model:\n",
    "    - `MAX_NEURONS` - the largest number of neurons possible in the first hidden layer\n",
    "    - `MIN_NEURONS` - the least number of neurons allowed in a hidden layer before no additional layers are added (the last hidden layer could have `neuronPct*(MIN_NEURONS+1)` neurons)\n",
    "    - `MAX_LAYERS` -  the most hidden layers that will be added to the model\n",
    "    - The activation function in each hidden layer, here: `PReLU()`\n",
    "    - The regularization strategy implemented in each hidden layer, here: `None`\n",
    "    - The activation function of the output layer, here: `linear`\n",
    "- Training and evaluating:\n",
    "    - Resampling strategy: *bootstrapping*\n",
    "        - `SPLITS` - the number of bootstrapping itterations to perform for each \n",
    "        - `TEST_SIZE` - the portion of the data to use as the test set in each bootstrap itteration\n",
    "     - Model fitting:\n",
    "        - `EPOCHS` - the maximum number of epochs to perform for each\n",
    "        - Early stopping\n",
    "            - `PATIENCE` - the number of epochs to complete without seeing improvement\n",
    "- Bayesian Optimization:\n",
    "    - `INIT_POINTS` - the number of random parameter values to probe before attempting to optimize\n",
    "    - `N_ITER` - the number of iterations to perform for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e89548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model bounds:\n",
    "MAX_NEURONS = 5000\n",
    "MIN_NEURONS = 25\n",
    "MAX_LAYERS = 10\n",
    "\n",
    "# Bootstrapping settings:\n",
    "SPLITS = 4\n",
    "TEST_SIZE = 0.2\n",
    "print_bootstrap_stats = True\n",
    "\n",
    "# Model fitting and early stopping settings:\n",
    "EPOCHS = 18\n",
    "PATIENCE = 9\n",
    "\n",
    "# Bayesian optimizer settings\n",
    "INIT_POINTS = 25\n",
    "N_ITER = 50\n",
    "OPTIMIZER_RANDOM_STATE = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bccb2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model generating function\n",
    "def generate_model(dropout_rate, neuronPct, neuronShrink):\n",
    "    # We start with some percent of 5000 starting neurons on \n",
    "    # the first hidden layer.\n",
    "    neuronCount = int(neuronPct * MAX_NEURONS)\n",
    "    \n",
    "    # Construct neural network\n",
    "    model = Sequential()\n",
    "\n",
    "    # So long as there would have been at least MIN_NEURONS neurons and \n",
    "    # fewer than MAX_LAYERS layers, create a new layer.\n",
    "    layer = 0\n",
    "    while neuronCount>MIN_NEURONS and layer<MAX_LAYERS:\n",
    "        # The first (0th) layer needs an input input_dim(neuronCount)\n",
    "        if layer==0:\n",
    "            model.add(Dense(neuronCount, \n",
    "                input_dim=x.shape[1], \n",
    "                activation=PReLU()))\n",
    "        else:\n",
    "            model.add(Dense(neuronCount, activation=PReLU())) \n",
    "        layer += 1\n",
    "\n",
    "        # Add dropout after each hidden layer except the last one\n",
    "        if layer<MAX_LAYERS-1:\n",
    "            model.add(Dropout(dropout_rate))\n",
    "\n",
    "        # Shrink neuron count for each layer\n",
    "        neuronCount = neuronCount * neuronShrink\n",
    "\n",
    "    # Add the output layer\n",
    "    ####################\n",
    "    # For regression:\n",
    "    model.add(Dense(1))\n",
    "    ####################\n",
    "    # For binary classification:\n",
    "    # model.add(Dense(y.shape[1],activation='sigmoid')) # Output\n",
    "    ####################\n",
    "    # For multiclassification:\n",
    "    # model.add(Dense(y.shape[1],activation='softmax')) # Output\n",
    "    ####################\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2f2c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluating function\n",
    "def evaluate_network(dropout_rate, neuronPct, neuronShrink, learning_rate):\n",
    "    # Set up bootstrapping\n",
    "    \n",
    "    # For Regression:\n",
    "    boot = ShuffleSplit(n_splits=SPLITS, test_size=TEST_SIZE)\n",
    "    # For Classification:\n",
    "    #boot = StratifiedShuffleSplit(n_splits=SPLITS, test_size=0.1)\n",
    "    \n",
    "    # Track progress\n",
    "    mean_benchmark = []\n",
    "    epochs_needed = []\n",
    "    num = 0\n",
    "    \n",
    "    # Loop through samples\n",
    "    ####################\n",
    "    #For regression:\n",
    "    for train, test in boot.split(x,y):\n",
    "    ####################\n",
    "    #For classification:\n",
    "    #for train, test in boot.split(x,df[y_column]):\n",
    "    ####################\n",
    "        start_time = time.time()\n",
    "        num+=1\n",
    "\n",
    "        # Split train and test\n",
    "        x_train = x[train]\n",
    "        y_train = y[train]\n",
    "        x_test = x[test]\n",
    "        y_test = y[test]\n",
    "\n",
    "        # Generate the model using the function we defined\n",
    "        model = generate_model(dropout_rate, neuronPct, neuronShrink)\n",
    "        \n",
    "        # Compile the model\n",
    "        \n",
    "        ####################\n",
    "        #For regression:\n",
    "        model.compile(loss='mean_squared_error',\n",
    "                      optimizer=Adam(learning_rate=learning_rate))\n",
    "        ####################\n",
    "        #For classification:\n",
    "        #model.compile(loss='categorical_crossentropy', \n",
    "        #              optimizer=Adam(learning_rate=learning_rate))\n",
    "        ####################\n",
    "        monitor = EarlyStopping(monitor='val_loss',\n",
    "                                min_delta=1e-3, \n",
    "                                patience=PATIENCE,\n",
    "                                verbose=0, \n",
    "                                mode='auto', \n",
    "                                restore_best_weights=True)\n",
    "\n",
    "        # Train on the bootstrap sample\n",
    "        model.fit(x_train,\n",
    "                  y_train,\n",
    "                  validation_data=(x_test,y_test),\n",
    "                  callbacks=[monitor],\n",
    "                  verbose=0,\n",
    "                  epochs=EPOCHS)\n",
    "        \n",
    "        # Keep track of needed epochs\n",
    "        epochs_used = monitor.stopped_epoch if monitor.stopped_epoch > 0 else EPOCHS\n",
    "        epochs_needed.append(epochs_used)\n",
    "\n",
    "        # Predict on the out of boot (validation)\n",
    "        pred = model.predict(x_test, \n",
    "                             verbose=0)\n",
    "\n",
    "        # Measure this bootstrap's log loss\n",
    "        ####################\n",
    "        # For regression:\n",
    "        score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "        ####################\n",
    "        # For classification:\n",
    "        #y_compare = np.argmax(y_test,axis=1) # For log loss calculation\n",
    "        #score = metrics.log_loss(y_compare, pred)\n",
    "        ####################\n",
    "        \n",
    "        mean_benchmark.append(score)\n",
    "        m1 = statistics.mean(mean_benchmark)\n",
    "        m2 = statistics.mean(epochs_needed)\n",
    "        mdev = statistics.pstdev(mean_benchmark)\n",
    "        \n",
    "        # Print stats for this iteration\n",
    "        if print_bootstrap_stats:\n",
    "            time_took = time.time() - start_time\n",
    "        \n",
    "            print(f\"#{num}: score (rmse)={score:.6f}, mean score={m1:.6f},\",\n",
    "                  f\" stdev={mdev:.6f}\",\n",
    "                  f\" epochs={epochs_used}, mean epochs={int(m2)}\",\n",
    "                  f\" time={hms_string(time_took)}\")\n",
    "        \n",
    "    tensorflow.keras.backend.clear_session()\n",
    "    \n",
    "    return (-m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1599ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Bayesian optimiztion\n",
    "\n",
    "# Supress NaN warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category = RuntimeWarning)\n",
    "\n",
    "# Bounded region of parameter space\n",
    "pbounds = {'dropout_rate': (0.0, 0.499),\n",
    "           'learning_rate': (0.0, 0.1),\n",
    "           'neuronPct': (0.01, 1),\n",
    "           'neuronShrink': (0.01, 1)\n",
    "          }\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=evaluate_network,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,  \n",
    "    # verbose = 1 prints only when a maximum is observed\n",
    "    # verbose = 0 is silent\n",
    "    random_state=OPTIMIZER_RANDOM_STATE,\n",
    ")\n",
    "\n",
    "# Store logs of the probes for later use\n",
    "# See docs for load_log tools\n",
    "logger = JSONLogger(path='logs.log',\n",
    "                    reset=True # False: don't clear existing log\n",
    "                   )\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "start_time = time.time()\n",
    "optimizer.maximize(init_points=INIT_POINTS, n_iter=N_ITER,)\n",
    "time_took = time.time() - start_time\n",
    "\n",
    "print(f\"Total runtime: {hms_string(time_took)}\")\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f619d06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
